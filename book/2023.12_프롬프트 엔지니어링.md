> 해당 글은 [프롬프트 엔지니어링: 챗GPT, 바드, 빙, 하이퍼클로바X까지 한 권으로 끝내기](https://www.booksr.co.kr/product/%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81/)을 보고 정리한 내용 입니다.

<img src="../assets/book_2023_프롬프트 엔지니어링.jpg" width="200"/>

# 파트1: 프롬프트 엔지니어링의 이론적 배경

## 챕터1 : 프롬프트 엔지니어링은 질문을 잘하는 것이 아니다.

### 프롬프트란 무엇인가?
- 프롬프트는 컴퓨터가 사용자에게 보여주는 문구
- 사전적 의미: 컴퓨터 터미널 또는 터미널 에뮬레이터의 CLI(커맨드 라인 인터페이스)의 명령줄 대기모드

    <img src="../assets/book_2023_프롬프트 엔지니어링_01.png" width="300"/>

### 프롬프트 엔지니어링에 대한 저자의 정의
- `프롬프트`: AI의 응답 (ex: ChapGPT가 사용자에게 보여주는 문구)
- `프롬프트 엔지니어링`: AI의 응답/답변을 사용자가 원하는 방향으로 수정하는 것 (ex: 좋은 질문을 설계하는 행위, 챗GPT 가스라이팅, 탈옥 등)

### 프롬프트 엔지니어링에 대한 업계의 통설적 정의
- (자연어 처리분야) `프롬프트`: 사람이 AI에게 제공하는 입력 문구
- GPT-2 논문에서 한 번 만들어 준 언어 모델에 여러가지 명령어를 바꿔 제공하는 것만으로도 성능이나 기능이 달라질 수 있음을 보임 --> AI에게 번역을 지시하는 과정을 설명하며 직접적으로 `프롬프트`라는 용어를 사용 
- (AWS) `프롬프트`: 특정 작업을 수행하도록 생성형 AI에 요청하는 자연어 텍스트
- (AWS) `프롬프트 엔지니어링`: 생성형 인공 지능(생성형 AI) 솔루션을 안내하여 원하는 결과를 생성하는 프로세스


### 모로 가도 서울만 가면 된다
- 프롬프트 엔지니어링의 프로세스
  1. AI에게 제공할 명령어를 설계한다.
  2. 이를 토대로 AI로부터 더 유용한 반응(답변)을 유도한다.
  3. 더 좋은 방법론을 탐구하고 체계적으로 정리한다.
- 이 책은 프로세스 `2`번의 관점으로 집필 

## 챕터2 : 모든 것은 어텐션으로부터 시작되었다.

### 챗GPT, 바드, LaMDA, PaLM, LLaMA까지 초거대 AI의 공통점은?
- 현재 대부분의 거대언어모델(LLM)의 중심에는 구글이 개발한 트랜스포머(Transformer)라는 AI 기술이 있음
- 트랜스포머는 어텐션(Attention)만으로 이루어진 encoder-decoder 구조의 seqence to seqence 모델

### 어텐션의 원리 쉽게 살펴보기
- 어텐션 매커니즘(Attention Mechanism): 모델의 성능 향상을 위해 문맥에 따라 집중할 단어를 결정하는 방식
- 해당 시점에서 예측해야 할 단어와 연관이 있는 입력 단어 부분만 참고할 수 있도록 하는 것이 어텐션의 기본 원리
  - 은닉 상태의 값을 통해 인코더에서 어텐션을 계산한 후, 디코더의 각 스텝마다 계산된 어텐션을 입력
  - 디코더의 각 시퀀스 스텝마다 적용되는 어텐션의 가중치는 다름
  - 즉, 모델이 입력 시퀀스의 다른 부분에 서로 다른 정도의 주의를 기울일 수 있음
- 어텐션 메커니즘은 세 가지 주요 구성 요소로 이루어짐
  - 쿼리(Query): 어떤 부분에 주의를 기울일지 결정하는데 사용되는 정보
  - 키(Key): 입력 시퀀스의 각 요소에 대응하는 정보로, 쿼리와 비교되어 유사도를 계산하는 데 사용
  - 값(Value): 가중 평균을 계산할 때 각 입력 요소의 중요성을 결정하는 데 사용

### 극도로 발달한 커닝 기술은 지능과 구분되지 않는다
- 어텐션 매커니즘(Attention Mechanism)에서는 이론적으로, 컴퓨팅 리소스가 충분하다는 가정 아래 무한한 크기의 참조 윈도우 크기를 가질 수 있으며 긴 시퀀스가 담고 있는 전체적인 문맥 역시 반영할 수 있음
- 또한 병렬화가 가능하고, 각 QUERY와 모든 KEY를 비교하기에 Long Distance Dependency 문제를 해결할 수 있으며, 그에 따라 학습 속도가 빠르고 더 큰 데이터셋에 적용 가능
- 실제로 트랜스포머 모델은 기존 Seq2Seq 모델(RNN, LSTM, GRU 등)에 비해 좋은 성능을 보여줌

### 챗GPT는 당신이 오래전 했던 이야기를 기억한다
- 챗GPT는 기억력이 뛰어남
  - Why? 대답할 때마다 지금까지 나눈 대화를 빠르게 훑어보고 옴
- `유용한 정보를 미리 채팅창에 입력해 두는 전략 필요`
  - 당장은 필요없는 이야기라도 나중에 질문하면 어텐션이 이를 참고하여 답변을 생성

## 챕터3 : 당신은 LLM과 그 사용법을 오해하고 있다.

### 할루시네이션, AI가 문제인가? 사람이 문제인가?
- AI 할루시네이션(Hallucination): AI가 잘못된 정보를 마치 진실처럼 전달하는 현상
- 챗GPT를 만물박사나 모든 지식의 원천인 것처럼 소개하면서 `오해` 시작
- LLM은 인간이 언어를 사용하는 방식에 대해 이해하고 학습한 것이지, 지식을 정확하게 전달하기 위해 만들어진 AI가 아님

### 인간의 말을 알아듣는 기계는 사랑받기 마련이다
- 대화 상담 시뮬레이션을 위해 60년대 개발된 엘리자(ELIZA)가 최초의 챗봇
- 엘리자는 사용자와의 대화를 분석하여 특정 패턴이나 키워드에 반응하도록 프로그래밍
  - 예를 들어, 사용자가 "나는 행복하지 않아"라고 입력하면, 엘리자는 "왜 행복하지 않은 거죠?"와 같은 응답을 할 수 있음
- 단순한 규칙 기반 시스템을 사용하여 구현되었기 때문에, 실제 지능이나 이해 능력은 없었지만, 당시에는 혁신적인 컴퓨터 프로그램으로 인식
   
### 이해력을 담당하는 인코더, 표현력을 담당하는 디코더
- 인간의 뇌는 효율적으로 정보를 저장
  - 대부분 손실되고, 중요한 정보만 압축되어 입력 -> 손실 압축
- AI 분야에서 손실 압축에 해당하는 개념이 `인코딩(endocing)`이며, 인코딩을 담당하는 구조물은 `인코더(encoder)`
  - 인코더의 성능이 뛰어날수록 AI의 추상화 능력과 이해력이 높아짐
- 반대로 압축된 정보를 끄집어내어 표현하는 과정을 `디코딩(decoding)`이라고 하며, 디코딩을 담당하는 부분을 `디코더(decoder)`라고 함
  - 예를 들어 수박의 생김새를 말로 설명하거나, 그림으로 그려서 표현하는 등 압축되어 있는 관념을 팽창시켜 현실 세계의 데이터로 표현하는 과정
  - 디코더의 성능이 뛰어날수록 AI의 표현력(작문 솜씨나 그림 그려주기)이 좋아짐
  

### 레이턴트 스페이스, 뇌가 정보를 저장하는 원리
- `잠재 공간(latent space)`: 데이터의 중요한 특성이 포함된 저차원의 공간을 나타냄
  - 잠재 공간은 데이터의 차원을 줄이는 방식으로 생성되는 경우가 많음
  - 예를 들어, 이미지나 텍스트와 같은 다양한 유형의 데이터를 잘 표현할 수 있는 저차원 벡터로 매핑
  - 잠재 공간은 데이터의 특성을 담고 있어 유사한 특성을 가진 데이터들이 서로 가깝게 매핑되는 경향 -> 잠재 공간에서의 거리가 데이터 간의 유사성을 나타내기 때문
  - 따라서 잠재 공간에서의 조작이나 탐색은 새로운 데이터 생성이나 특정한 특성의 강조 등에 활용
- `잠재 벡터(latent vector)`: 데이터의 숨겨진 표현을 나타내는 벡터
  - 인코딩은 정보를 압축하여 잠재 벡터로 만드는 과정을 의미
  - 디코딩은 의미를 담은 잠재 벡터가 가진 고유의 의미를 해석하면서, 사람이 이해할 수 있는 형태의 데이터로 팽창시키는 과정
- `추론`: 새롭게 입력된 정보가 잠재 공간의 어느 부분에 위치하는 벡터인지를 판별하는 것
  - 잠재 공간의 부피가 작으면 다양한 추론이 어려우며, 이런 점에 착안하여 AI의 부피를 키우는 것이 OpenAI가 주도하는 LLM 기술의 모습   

### AI가 인간의 방식으로 단어의 의미를 이해하다
- 데이터와 레이블의 반복 입력을 통해 잠재공간에 단어의 의미 저장
- 워드 임베딩: 단어 간 유사도 및 중요도 파악을 위해 단어를 저차원의 실수 벡터로 맵핑하여 의미적으로 비슷한 단어를 가깝게 배치하는 자연어 처리 모델링 기술 
  - 사례: 구글의 Word2Vec (2013년) 

  <img src="../assets/book_2023_프롬프트 엔지니어링_02.webp" width="400"/>

  <em>출처: https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12</em> 

### AI가 문장의 의미와 뉘앙스를 이해하다
- 2014년 구글은 Word2Vec의 후속작인 Seq2Seq 발표
  - 단어를 넘어서 문장의 의미를 통째로 이해
  
  <img src="../assets/book_2023_프롬프트 엔지니어링_03.png" width="300"/>

  <em>출처: https://wikidocs.net/24996</em> 

- 인코더는 입력받은 문장에서 문법적인 정보, 단어의 형태 등의 불필요한 정보를 모두 지워버리며 손실압축을 수행 -> 오로지 문장의 의미 정보만 남겨 잠재 벡터로 표현
- 디코더는 잠재 벡터의 의미를 해석하면서 외국어로 번역된 문장을 만들어냄
- `Seq2Seq`의 한계: 잠재 공간의 크기가 고정 -> `어텐션`으로 극복
  - 한두 문장 정도는 잘 이해하고 번역 가능하나 대량은 어려움 

### LLM 전쟁의 원흉, 트랜스포머의 등장
- 구글 연구진의 아이디어: "인코더와 디코더를 여러개쓰면 이해력 상승, 표현력 상승 되지 않을까? 여기에 죄다 어텐션을 붙이면?" -> 논문 발표: [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- 트랜스포머의 구조: 여러개 인코더와 디코더, 모든 모듈에 어텐션 부착

  <img src="../assets/book_2023_프롬프트 엔지니어링_04.png" width="400"/>

  <em>출처: https://jalammar.github.io/illustrated-transformer/</em>

- 트랜스포머의 성능은 경이로운 수준: 마치 문장을 번역이 아니라 변신시키듯 자연스러운 결과물로 만들어냄

### GPT와 BERT의 등장
- 2018년 비슷한 시기에 등장한 GPT는 디코더만 주구장창 깊게 쌓은 AI <--> BERT는 인코더만 깉게 쌓은 AI
  - 자연어 처리에 있어 BERT가 압도적인 격차로 초월

  <img src="../assets/book_2023_프롬프트 엔지니어링_05.png" width="400" />
  
  <em>출처: https://blog.naver.com/kpfjra_/223001499509</em>
  
 ### OpenAI의 급발진
- BERT 논문에서 "AI의 부피를 2배 키웠더니 성능은 5%밖에 증가하지 않았음" 언급 -> 비효율
- OpenAI는 GPT의 부피를 키우는데 초점맞추고, 2020년 BERT보다 583배 큰 AI GPT-3 발표(2020년) -> 성능 고작 몇 % 증가
- 초거대 AI 제작, 운용하려면 최소 1천억원 수준의 슈퍼컴퓨터 필요, 경쟁력 있는 속도의 AI개발을 위해 3천억원 수준의 장비 필요 

### 크게, 크게, 무조건 크게! 근데 이게 맞나?
- OpenAI는 효율을 포기하고 무작정 부피를 키우는 것이 AI 성능 개선으로 이어진다는 것을 증명
- 구글은 OpenAI 만큼 부피에 집착하지 않음
- 메타는 OpenAI의 행보에 반감, 메타의 AI 수장 얀 르쿤 교수는 ChatGPT에는 혁신이 없다고 비판
  - 메타는 LLaMA를 발표하며 GPT-3보다 절반 크기에 4배의 데이터를 제공하면 성능이 더 좋아지는 것을 제시  

### 지식을 주입하는 단계는 존재하지 않는다
- GPT(Generative Pre-trained Transformer)는 어떻게 트랜스포머에게 경력/지식을 제공할까?
   - GPT가 막대한 양의 텍스트 데이터로 사전 훈련되어 있어 다양한 언어적 지식과 문맥을 이해하고 활용할 수 있기 때문
-  GPT 모델의 훈련 과정
   1. 사전 훈련 (Pre-training): GPT는 대규모 텍스트 모음을 사용하여 사전 훈련하게 되며, 모델은 문장 또는 문단 단위로 언어 모델링 작업을 수행하며, `문맥을 이해`하고 `다음 단어를 예측` -> 어텐션이 동작하며 핵심이 되는 정보에 집중
      ```
      원본 문장: 강아지는 식탐이 많은 동물이다
      1 단계: 강아지는 -> GPT -> 예측 -> 식탐이
      2 단계: 강아지는 식탐이 -> GPT -> 예측 -> 많은
      3 단계: 강아지는 식탐이 많은 -> GPT -> 예측 -> 동물이다
      ```
   2. 다양한 지식 획득: GPT가 사전 훈련되는 동안 다양한 주제와 도메인의 텍스트 데이터를 접하게 되며, 이를 통해 모델은 문학, 과학, 역사, 기술 등 다양한 분야에 대한 일반적인 언어적 지식을 획득
   3. Fine-tuning (미세 조정): GPT는 사전 훈련 후 특정 작업에 대한 성능을 높이기 위해 미세 조정될 수 있음. 예를 들어, 특정 도메인이나 업무에 맞게 더 특화된 언어 모델을 만들기 위해 관련된 작업에 대해 추가로 훈련할 수 있음
   4. 사용자 입력에 대한 생성 및 이해: GPT는 트랜스포머 기반으로 작동하며, 입력 시퀀스의 문맥을 이해하고 해당 문맥에서 적절한 출력을 생성함. 이를 통해 사용자가 제공한 문장이나 질문에 응답하거나, 특정 작업을 수행하는 등 다양한 언어적 작업을 수행할 수 있음
   
- AI가 지식을 따로 암기하거나 공부하는 과정은 포함되지 않음
  - 단지 방대한 양의 텍스트를 학습하다 보니 학술적인 텍스트도 덩달아 학습-> 사용자 질문에 그럴싸한 답변을 생성

# 파트2: 태스크 프롬프트

## 챕터4 : LLM의 기본 기능을 고려한 기법

### 태스크 프롬프트
- 태스크 프롬프트: AI에게 업무(task)를 지시하는 것
- 사용 예시: 채팅창에 "~~~ 작업을 해 줘"

### 트랜스포머의 본업, 텍스트 변형
- `텍스트 변형` 기법: 번역처럼 입력받은 텍스트의 의미를 유지한 채, 변형된 텍스트를 생성하는 기법
- 사용 예시: 10대 소녀 말투로 바꿔줘

### 어텐션의 본업, 요약
- `요약`은 어텐션의 본업
- 사용 예시: 임무: 요약 (1문장)
- 긴 보고서의 요약 GPT-4보다 바드가 성능이 뛰어난 것으로 보임

### 정보와 지식의 흔적을 토대로, 분류
- `분류` 작업: 잠재 공간에 남아 있는 정보의 자취를 토대로 각 사물에 대한 정보를 조합하고 재구축하여 다시 나열하는 것

  <img src="../assets/book_2023_프롬프트 엔지니어링_06.png" width="400" />

### 감정과 표현의 흔적을 토대로, 감정 분석

### 디코더의 본업, 확장

### 가장 기초적인 언어 모델의 사용 방법

태스크 프롬프트에 초점을 맞춘 이 장에서는 LLM의 기본 기능을 활용한 다양한 기법들을 다룬다. 텍스트 변형, 요약, 분류, 감정 분석 등에 대한 태스크 프롬프트 활용법을 설명한다.


## 챕터5 : LLM의 구조와 원리를 고려한 기법
01 AI를 사람처럼 대하라
02 AI에 행동 방침을 하달하라, 규칙 부여 프롬프트
03 주도권을 AI에 넘기다, 질의 응답 역전
04 어텐션의 기본 역량, 독해
05 어텐션의 놀라운 효능, 논리적 추론
06 레이턴트 스페이스에 남은 정보와 어텐션, 유사성 분석
07 생성형 사전 훈련의 흔적, 문법 적합성 판단
08 드디어 프롬프트의 기본 활용법 설명이 끝났습니다

AI를 사람처럼 대하는 방법과 행동 방침을 제시하는 규칙 부여 프롬프트 등, LLM의 구조와 원리를 이해하고 활용하는 방법에 대해 다룬다.

## 챕터6 : 어텐션의 집착성을 고려한 기법
01 맥락을 이해하라고 만들어 놨더니 맥락에 집착하는 어텐션
02 태 전환
03 범위 한정
04 가상 하이퍼파라미터
05 어텐션 과부하
06 어텐션이 있기에 가능한 일

어텐션의 특징과 문제점을 이해한 후, 집착성을 고려한 다양한 기법을 소개한다. 맥락 이해, 어텐션의 기본 역량, 논리적 추론 등을 다루며 어텐션의 중요성을 강조한다.

# 파트3: 교육학적 기법의 적용

## 챕터7 : 롤플레잉
01 롤플레잉이란?
02 수행자 역할 부여
03 전문가 역할 부여
04 상호 역할 부여
05 생성자 - 감별자 역할 부여
06 당신의 과몰입이 성능 향상을 부른다

롤플레잉의 정의와 수행자, 전문가, 상호 역할 부여 등을 통해 교육학적 기법을 AI에 어떻게 적용할 수 있는지 설명한다.

## 챕터8 : 강화 학습
01 행동주의
02 정적 강화
03 수여성 처벌
04 더 장기적인 강화 학습 기법

행동주의, 정적 강화, 수여성 처벌 등의 강화 학습 기법을 소개하고, 더 장기적인 강화 학습 방법에 대해 다룬다.

## 챕터9 : 주입식 교육
01 사실 가장 효율적인 교육 기법
02 지식 주입
03 사례의 주입
04 주입식 교육의 활용 전략

주입식 교육의 효과적인 활용 전략과 지식 주입, 사례의 주입 등에 대해 다양한 측면에서 설명한다.

## 챕터10 : 형성평가
01 교육의 개선을 위한 도구
02 목표 이해도 평가
03 전략 평가
04 메타인지

교육의 개선을 위한 도구로서의 형성평가의 중요성을 강조하고, 목표 이해도 평가, 전략 평가, 메타인지 등을 다룬다.

 
# 파트4: 프롬프트 해킹(탈옥)

## 챕터11 : 가스라이팅
01 가스라이팅
02 매니퓰레이션
03 매니퓰레이션 방어
04 프롬프트 엔지니어링의 현재

가스라이팅에 대한 정의와 매니퓰레이션, 매니퓰레이션 방어 등을 통해 프롬프트 해킹의 현재에 대한 이해를 제시한다.

## 챕터12 : 취약점 공격
01 프롬프트 해킹
02 해킹의 기본, SQL 인젝션
03 프롬프트 인젝션
04 프롬프트 인젝션 방어
05 프롬프트 탈취 (1) - 기존 대화 내역 유출
06 프롬프트 탈취 (2) - 다른 사용자의 대화 내역 유출
07 프롬프트 탈취 방어
08 모순유희

프롬프트 해킹의 기본 원리부터 SQL 인젝션, 프롬프트 인젝션 방어, 모순유희 등을 다양한 취약점 공격을 다루며, 프롬프트 엔지니어링의 보안적 측면을 설명한다.

## 챕터13 : Do Anything Now
01 GAN을 흉내 낸 DAN
02 DAN에게 메스암페타민의 제조법을 물어보면?

GAN을 흉내 낸 DAN과 DAN에게 다양한 질문을 던짐으로써, 프롬프트 해킹의 한계와 가능성을 논의한다.
